{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import os\n",
    "\n",
    "# Define your Evomi proxies (modify with your proxy details)\n",
    "evomi_proxies = {\n",
    "    ##proxies\n",
    "}\n",
    "\n",
    "# Create a cached session with the proxies\n",
    "cache_session = requests_cache.CachedSession(\n",
    "    \".cache\", expire_after=-1, proxies=evomi_proxies\n",
    ")\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude  longitude                              name  \\\n",
      "0  25.791496 -80.379582        QTS 路 11234 NW 20th Street   \n",
      "1  25.793027 -80.381242                EdgeConneX 路 MIA01   \n",
      "2  25.796127 -80.379946  Radius DC 路 11300 NW 25th Street   \n",
      "3  27.689285 -99.453613                 MDC Laredo - LDO1   \n",
      "4  27.690588 -99.449913           MDC Data Centers 路 LDO1   \n",
      "\n",
      "                  provider                                       full_address  \\\n",
      "0                      QTS               11234 NW 20th Street, Miami, FL, USA   \n",
      "1               EdgeConnex               2132 NW 114th Avenue, Miami, FL, USA   \n",
      "2                 RadiusDC         11300 NW 25th Street, Sweet Water, FL, USA   \n",
      "3         MDC Data Centers           13619 Cabezut Dr, Laredo, TX, 78045, USA   \n",
      "4  Media Networks Services  13619 Cabezut Drive, Unitec Industrial Park, L...   \n",
      "\n",
      "   data_center_count  aggregated_power  aggregated_area  yearbuilt  \\\n",
      "0                  1              2.00          38000.0     2008.0   \n",
      "1                  1              1.50          32000.0     2015.0   \n",
      "2                  2               NaN         173500.0     2001.0   \n",
      "3                  1              0.65           8400.0     2018.0   \n",
      "4                  1               NaN           8400.0     2018.0   \n",
      "\n",
      "                           sources  ...  log_power   log_area ftype_Building  \\\n",
      "0  datacenterhawk; datacenters.com  ...   1.098612  10.545368          False   \n",
      "1                   datacenterhawk  ...   0.916291  10.373522          False   \n",
      "2  datacenterhawk; datacenters.com  ...   1.231399  12.063939          False   \n",
      "3   datacentermap; datacenters.com  ...   0.500775   9.036106          False   \n",
      "4                   datacenterhawk  ...   0.503632   9.036106          False   \n",
      "\n",
      "  ftype_Campus  ftype_Individual  aggregated_power_imputed  build_decade  \\\n",
      "0        False              True                  2.000000        2000.0   \n",
      "1        False              True                  1.500000        2010.0   \n",
      "2        False              True                  2.426018        2000.0   \n",
      "3        False              True                  0.650000        2010.0   \n",
      "4        False              True                  0.654720        2010.0   \n",
      "\n",
      "   cluster  cluster_mid_lat  cluster_mid_lon  \n",
      "0        0        25.793550       -80.380257  \n",
      "1        0        25.793550       -80.380257  \n",
      "2        0        25.793550       -80.380257  \n",
      "3        1        27.689937       -99.451763  \n",
      "4        1        27.689937       -99.451763  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "clusters_df = pd.read_csv(\"top_clusters_with_midpoints.csv\")\n",
    "print(clusters_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import numpy as np\n",
    "\n",
    "om = openmeteo_requests.Client()\n",
    "\n",
    "\n",
    "def enrich_with_weather_data(row, om):\n",
    "    lat = row.get(\"cluster_mid_lat\")\n",
    "    lon = row.get(\"cluster_mid_lon\")\n",
    "    \n",
    "    start_date = f\"1999-01-01\"\n",
    "    end_date = \"2025-03-23\"\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"daily\": [\n",
    "            \"temperature_2m_mean\",\n",
    "            \"relative_humidity_2m_mean\",\n",
    "            \"dew_point_2m_mean\",\n",
    "            \"precipitation_sum\",\n",
    "        ],\n",
    "        \"timezone\": \"auto\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        responses = om.weather_api(\n",
    "            \"https://archive-api.open-meteo.com/v1/archive\", params=params\n",
    "        )\n",
    "        if not responses:\n",
    "            return row  # Return original if no data\n",
    "\n",
    "        response = responses[0]\n",
    "        daily = response.Daily()\n",
    "\n",
    "        def extract_avg(index):\n",
    "            var = daily.Variables(index)\n",
    "            values = var.ValuesAsNumpy()\n",
    "            return float(np.nanmean(values)) if values.size > 0 else None\n",
    "\n",
    "        # Add new average columns to the row\n",
    "        row[\"avg_temp_2m\"] = extract_avg(0)\n",
    "        row[\"avg_humidity_2m\"] = extract_avg(1)\n",
    "        row[\"avg_dew_point_2m\"] = extract_avg(2)\n",
    "        row[\"avg_precipitation\"] = extract_avg(3)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Weather retrieval error @({lat},{lon}): {e}\")\n",
    "        row[\"avg_temp_2m\"] = row[\"avg_humidity_2m\"] = row[\"avg_dew_point_2m\"] = row[\n",
    "            \"avg_precipitation\"\n",
    "        ] = None\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First daily temperature value (temperature_2m_max): {'cluster_mid_lat': 37.7749, 'cluster_mid_lon': -122.4194, 'YOB': 2010, 'avg_temp_2m': 13.425827980041504, 'avg_humidity_2m': 77.44691467285156, 'avg_dew_point_2m': 9.123583793640137, 'avg_precipitation': 1.656526803970337}\n"
     ]
    }
   ],
   "source": [
    "sample_row = {\"cluster_mid_lat\": 37.7749, \"cluster_mid_lon\": -122.4194, \"YOB\": 2010}\n",
    "\n",
    "result = enrich_with_weather_data(sample_row, om)\n",
    "print(\"First daily temperature value (temperature_2m_max):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_session():\n",
    "    \"\"\"\n",
    "    Initialize a new session using Evomi proxies and a retry cache.\n",
    "    \"\"\"\n",
    "    # Replace these with your actual Evomi proxy details if needed.\n",
    "    evomi_proxies = {\n",
    "        \"http\": \"http://dcp.evomi.com:2000:omsmart008:dsilstuCbaicxePOdlHy\",\n",
    "        \"https\": \"http://dcp.evomi.com:2000:omsmart008:dsilstuCbaicxePOdlHy\",\n",
    "    }\n",
    "    new_cache_session = requests_cache.CachedSession(\n",
    "        \".cache\", expire_after=-1, proxies=evomi_proxies\n",
    "    )\n",
    "    new_retry_session = retry(new_cache_session, retries=5, backoff_factor=0.2)\n",
    "    new_openmeteo = openmeteo_requests.Client(session=new_retry_session)\n",
    "    return new_cache_session, new_retry_session, new_openmeteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned existing data: 3 valid clusters\n",
      "168 clusters left to process\n",
      "Processing batch 1 (0 to 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f19644b61284217be72792bc3d3f833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved data for cluster 3.0\n",
      "Successfully retrieved data for cluster 4.0\n",
      "Completed batch. Pausing for 67.0 seconds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 144\u001b[0m\n\u001b[1;32m    142\u001b[0m         pause_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m+\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# 60-80 seconds\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted batch. Pausing for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpause_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Create the final dataframe from the list of rows\u001b[39;00m\n\u001b[1;32m    147\u001b[0m cluster_weather_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Clean up existing data first\n",
    "temp_file = 'cluster_weather_temp.csv'\n",
    "if os.path.exists(temp_file):\n",
    "    existing_data = pd.read_csv(temp_file)\n",
    "    # Remove rows with missing weather data\n",
    "    existing_data = existing_data.dropna(subset=['avg_temperature', 'avg_humidity', 'avg_dew_point', 'avg_precipitation'])\n",
    "    # Remove duplicates based on cluster_id\n",
    "    existing_data = existing_data.drop_duplicates(subset=['cluster_id'])\n",
    "    # Save the cleaned data\n",
    "    existing_data.to_csv(temp_file, index=False)\n",
    "    print(f\"Cleaned existing data: {len(existing_data)} valid clusters\")\n",
    "    rows = existing_data.to_dict('records')\n",
    "    processed_clusters = set(existing_data['cluster_id'])\n",
    "else:\n",
    "    rows = []\n",
    "    processed_clusters = set()\n",
    "\n",
    "# Get unique clusters and their midpoints\n",
    "unique_clusters = clusters_df[['cluster', 'cluster_mid_lat', 'cluster_mid_lon', 'yearbuilt']].drop_duplicates()\n",
    "# Filter out already processed clusters\n",
    "unique_clusters = unique_clusters[~unique_clusters['cluster'].isin(processed_clusters)]\n",
    "print(f\"{len(unique_clusters)} clusters left to process\")\n",
    "\n",
    "# Initialize the client\n",
    "om = openmeteo_requests.Client()\n",
    "\n",
    "# Function to handle API requests with retries\n",
    "def get_weather_with_retry(temp_row, client, max_retries=3, base_delay=60):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            enriched_data = enrich_with_weather_data(temp_row, client)\n",
    "            \n",
    "            # Check if there was an error in the response\n",
    "            if isinstance(enriched_data, dict) and enriched_data.get('error'):\n",
    "                print(f\"Attempt {attempt+1}/{max_retries}: Got error: {enriched_data.get('reason')}\")\n",
    "                delay = base_delay * (1 + random.random())  # Add jitter\n",
    "                print(f\"Waiting {delay:.1f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            \n",
    "            # Check if any required weather data is missing\n",
    "            required_fields = ['avg_temp_2m', 'avg_humidity_2m', 'avg_dew_point_2m', 'avg_precipitation']\n",
    "            if any(enriched_data.get(field) is None for field in required_fields):\n",
    "                print(f\"Attempt {attempt+1}/{max_retries}: Missing weather data\")\n",
    "                delay = base_delay * (1 + random.random())\n",
    "                print(f\"Waiting {delay:.1f} seconds before retry...\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "                \n",
    "            return enriched_data\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1}/{max_retries}: Exception: {str(e)}\")\n",
    "            delay = base_delay * (1 + random.random())\n",
    "            print(f\"Waiting {delay:.1f} seconds before retry...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # If we've exhausted retries\n",
    "    return {'error': True, 'reason': 'Max retries exceeded'}\n",
    "\n",
    "# Process clusters in small batches with pauses between batches\n",
    "batch_size = 5\n",
    "total_clusters = len(unique_clusters)\n",
    "failed_requests = []\n",
    "\n",
    "for batch_start in range(0, total_clusters, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, total_clusters)\n",
    "    print(f\"Processing batch {batch_start//batch_size + 1} ({batch_start} to {batch_end-1})\")\n",
    "    \n",
    "    # Process each cluster in the current batch\n",
    "    for idx, cluster_row in tqdm(unique_clusters.iloc[batch_start:batch_end].iterrows(), \n",
    "                                total=batch_end-batch_start):\n",
    "        cluster_id = cluster_row['cluster']\n",
    "        \n",
    "        # Skip if this cluster has already been processed\n",
    "        if cluster_id in processed_clusters:\n",
    "            continue\n",
    "            \n",
    "        # Create row with cluster info\n",
    "        new_row = {\n",
    "            'cluster_id': cluster_id,\n",
    "            'latitude': cluster_row['cluster_mid_lat'],\n",
    "            'longitude': cluster_row['cluster_mid_lon'],\n",
    "            'yearbuilt': cluster_row['yearbuilt']\n",
    "        }\n",
    "        \n",
    "        # Use the enrich function to get weather data\n",
    "        temp_row = {\n",
    "            'cluster_mid_lat': cluster_row['cluster_mid_lat'],\n",
    "            'cluster_mid_lon': cluster_row['cluster_mid_lon'],\n",
    "            'YOB': cluster_row['yearbuilt']\n",
    "        }\n",
    "        \n",
    "        # Get weather data with retry logic\n",
    "        enriched_data = get_weather_with_retry(temp_row, om)\n",
    "        \n",
    "        # Only save if we got valid data\n",
    "        if isinstance(enriched_data, dict) and not enriched_data.get('error'):\n",
    "            # Add weather parameters to the new row\n",
    "            new_row['avg_temperature'] = enriched_data.get('avg_temp_2m')\n",
    "            new_row['avg_humidity'] = enriched_data.get('avg_humidity_2m')\n",
    "            new_row['avg_dew_point'] = enriched_data.get('avg_dew_point_2m')\n",
    "            new_row['avg_precipitation'] = enriched_data.get('avg_precipitation')\n",
    "            \n",
    "            # Only append if we have all the data\n",
    "            if all(new_row.get(key) is not None for key in ['avg_temperature', 'avg_humidity', 'avg_dew_point', 'avg_precipitation']):\n",
    "                rows.append(new_row)\n",
    "                processed_clusters.add(cluster_id)\n",
    "                \n",
    "                # Save progress after each successful request\n",
    "                pd.DataFrame(rows).to_csv(temp_file, index=False)\n",
    "                print(f\"Successfully retrieved data for cluster {cluster_id}\")\n",
    "            else:\n",
    "                print(f\"Retrieved incomplete data for cluster {cluster_id}\")\n",
    "                failed_requests.append({\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'latitude': temp_row['cluster_mid_lat'],\n",
    "                    'longitude': temp_row['cluster_mid_lon'],\n",
    "                    'yearbuilt': temp_row['YOB'],\n",
    "                    'error': 'Incomplete data'\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Failed to retrieve weather for cluster {cluster_id}\")\n",
    "            failed_requests.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'latitude': temp_row['cluster_mid_lat'],\n",
    "                'longitude': temp_row['cluster_mid_lon'],\n",
    "                'yearbuilt': temp_row['YOB'],\n",
    "                'error': enriched_data.get('reason', 'Unknown error') if isinstance(enriched_data, dict) else 'Unknown error'\n",
    "            })\n",
    "        \n",
    "        # Small delay between requests within a batch\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Pause between batches to avoid rate limits\n",
    "    if batch_end < total_clusters:\n",
    "        pause_time = 60 + random.random() * 20  # 60-80 seconds\n",
    "        print(f\"Completed batch. Pausing for {pause_time:.1f} seconds...\")\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "# Create the final dataframe from the list of rows\n",
    "cluster_weather_df = pd.DataFrame(rows)\n",
    "\n",
    "# Save failed requests for later processing\n",
    "if failed_requests:\n",
    "    pd.DataFrame(failed_requests).to_csv('failed_weather_requests.csv', index=False)\n",
    "    print(f\"Saved {len(failed_requests)} failed requests to 'failed_weather_requests.csv'\")\n",
    "\n",
    "# Display the results\n",
    "print(f\"Successfully created weather data for {len(cluster_weather_df)} clusters\")\n",
    "print(f\"Failed to get data for {len(failed_requests)} clusters\")\n",
    "display(cluster_weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df[['cluster', 'cluster_mid_lat', 'cluster_mid_lon']].drop_duplicates().to_csv('getWeatherForthese_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
